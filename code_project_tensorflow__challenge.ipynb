{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_project_tensorflow _challenge.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataplusplus-ai/Data-Science/blob/master/code_project_tensorflow__challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbWqwRQ7MHzb"
      },
      "source": [
        "import pandas as pd\n",
        "wine_names = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315', 'Proline']\n",
        "wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', names = wine_names)\n",
        "wine_df = pd.DataFrame(wine_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuKe3gD_V55e"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eguhfMtLMJym",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "cddcac27-2168-4bf5-ca57-10847340cb08"
      },
      "source": [
        "wine_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.938202</td>\n",
              "      <td>13.000618</td>\n",
              "      <td>2.336348</td>\n",
              "      <td>2.366517</td>\n",
              "      <td>19.494944</td>\n",
              "      <td>99.741573</td>\n",
              "      <td>2.295112</td>\n",
              "      <td>2.029270</td>\n",
              "      <td>0.361854</td>\n",
              "      <td>1.590899</td>\n",
              "      <td>5.058090</td>\n",
              "      <td>0.957449</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>746.893258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.775035</td>\n",
              "      <td>0.811827</td>\n",
              "      <td>1.117146</td>\n",
              "      <td>0.274344</td>\n",
              "      <td>3.339564</td>\n",
              "      <td>14.282484</td>\n",
              "      <td>0.625851</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>0.572359</td>\n",
              "      <td>2.318286</td>\n",
              "      <td>0.228572</td>\n",
              "      <td>0.709990</td>\n",
              "      <td>314.907474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>278.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.362500</td>\n",
              "      <td>1.602500</td>\n",
              "      <td>2.210000</td>\n",
              "      <td>17.200000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>1.742500</td>\n",
              "      <td>1.205000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>3.220000</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>1.937500</td>\n",
              "      <td>500.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.050000</td>\n",
              "      <td>1.865000</td>\n",
              "      <td>2.360000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>2.355000</td>\n",
              "      <td>2.135000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>4.690000</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>2.780000</td>\n",
              "      <td>673.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>13.677500</td>\n",
              "      <td>3.082500</td>\n",
              "      <td>2.557500</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>2.875000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>1.120000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>985.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>14.830000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.230000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>3.880000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>3.580000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1680.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Class     Alcohol  Malic acid         Ash  Alcalinity of ash  \\\n",
              "count  178.000000  178.000000  178.000000  178.000000         178.000000   \n",
              "mean     1.938202   13.000618    2.336348    2.366517          19.494944   \n",
              "std      0.775035    0.811827    1.117146    0.274344           3.339564   \n",
              "min      1.000000   11.030000    0.740000    1.360000          10.600000   \n",
              "25%      1.000000   12.362500    1.602500    2.210000          17.200000   \n",
              "50%      2.000000   13.050000    1.865000    2.360000          19.500000   \n",
              "75%      3.000000   13.677500    3.082500    2.557500          21.500000   \n",
              "max      3.000000   14.830000    5.800000    3.230000          30.000000   \n",
              "\n",
              "        Magnesium  Total phenols  Flavanoids  Nonflavanoid phenols  \\\n",
              "count  178.000000     178.000000  178.000000            178.000000   \n",
              "mean    99.741573       2.295112    2.029270              0.361854   \n",
              "std     14.282484       0.625851    0.998859              0.124453   \n",
              "min     70.000000       0.980000    0.340000              0.130000   \n",
              "25%     88.000000       1.742500    1.205000              0.270000   \n",
              "50%     98.000000       2.355000    2.135000              0.340000   \n",
              "75%    107.000000       2.800000    2.875000              0.437500   \n",
              "max    162.000000       3.880000    5.080000              0.660000   \n",
              "\n",
              "       Proanthocyanins  Color intensity         Hue  OD280/OD315      Proline  \n",
              "count       178.000000       178.000000  178.000000   178.000000   178.000000  \n",
              "mean          1.590899         5.058090    0.957449     2.611685   746.893258  \n",
              "std           0.572359         2.318286    0.228572     0.709990   314.907474  \n",
              "min           0.410000         1.280000    0.480000     1.270000   278.000000  \n",
              "25%           1.250000         3.220000    0.782500     1.937500   500.500000  \n",
              "50%           1.555000         4.690000    0.965000     2.780000   673.500000  \n",
              "75%           1.950000         6.200000    1.120000     3.170000   985.000000  \n",
              "max           3.580000        13.000000    1.710000     4.000000  1680.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApGW0_y8MeZf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b91c530d-3847-403e-b8a6-c3be64aa1686"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "const1 = tf.constant([[1,2,3], [1,2,3]]);\n",
        "const2 = tf.constant([[3,4,5], [3,4,5]]);\n",
        "\n",
        "result = tf.add(const1, const2);\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  output = sess.run(result)\n",
        "  print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4 6 8]\n",
            " [4 6 8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIC6XqchMy2O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujiARXjKN3Je"
      },
      "source": [
        "\n",
        "By Dante Sblendorio\n",
        "\n",
        "What is a neural net, and how can you create one? Keep reading to find out. Below, I explain the basics of setting up a neural net using TensorFlow.\n",
        "\n",
        "A neural net consists of three key components: the input layer, the hidden layer(s), and the output layer. Each layer has an arbitrary number of nodes (or neurons). In the example in the previous section, the input layer is const1 and const2. The matrix addition can be thought of as the hidden layer, and the output layer is the output. In the case of our wine data, the input data is the 13 chemical features and the output layer is the Class. The hidden layer can be thought of as a sophisticated ensemble of mathematical functions that behave as filters, extracting the relevant features for determining the correct Class. The structure of the neural net is inspired by biology, specifically the neural connections in the human brain. For the sake of this article, I won’t go into depth about the mathematical structure of the hidden layer(s). It is sufficient to think of it as a mathematical “black box” that extracts hidden meaning from the data. (However, if you want to learn more, this is a thorough online textbook on neural networks and deep learning.)\n",
        "\n",
        "We split the dataset into a training and test set. This allows us to “train” the mathematical operators within the hidden layer to converge on ideal values that correctly predict the correct Class based on the 13 features for each observation. We then inject the test set into the neural net and evaluate the accuracy to determine how well the net has been trained. Splitting the data in this way provides a way to avoid overfitting or underfitting the data, thereby giving a true estimation of the accuracy of the net. In order to prepare the data for TensorFlow, we perform some slight manipulation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvQ-pAAJN42Z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "#this prepares our Class variable for the NN\n",
        "def convertClass(val):\n",
        "   if val == 1:\n",
        "       return [1, 0, 0]\n",
        "   elif val == 2:\n",
        "       return [0, 1, 0]\n",
        "   else:\n",
        "       return [0, 0, 1]\n",
        "   \n",
        "Y = wine_df.loc[:,'Class'].values\n",
        "Y = np.array([convertClass(i) for i in Y])\n",
        "X = wine_df.loc[:,'Alcohol':'Proline'].values\n",
        "\n",
        "#we split the dataset into a test and training set\n",
        "train_x, test_x, train_y, test_y = train_test_split(X,Y , test_size=0.3, random_state=0)\n",
        "train_x = train_x.transpose()\n",
        "train_y = train_y.transpose()\n",
        "test_x = test_x.transpose()\n",
        "test_y = test_y.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPgnB_AdOE1-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc37KOXdOsR_"
      },
      "source": [
        "Now that the data is prepped, we define several functions that form the foundation of the neural net. First, we define a function that establishes the initial parameters of our net. Here we also define how many nodes are in each of the hidden layers (I chose to have two hidden layers). Since we have three possible values for Class, we have three nodes in the output layer. Next we define a forward propagation function. All this does is send the 13 features through the net, and subject them to the mathematical operations within the hidden layer.\n",
        "\n",
        "We also need to define a cost function. This is a critical function that allows us to “train” the network. It is a single value that describes how well the net does at predicting the correct Class. If the cost value is high, the mathematical operates adjust, and the data is fed through the net again. The data is fed through the net until the cost value converges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POt4K-XUOtd7"
      },
      "source": [
        "def init_parameters(num_input_features):\n",
        "\n",
        "   num_hidden_layer =  32\n",
        "   num_hidden_layer_2 = 16\n",
        "   num_output_layer_1 = 3\n",
        "   \n",
        "   tf.set_random_seed(1)\n",
        "   W1 = tf.get_variable(\"W1\", [num_hidden_layer, num_input_features], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "   b1 = tf.get_variable(\"b1\", [num_hidden_layer, 1], initializer = tf.zeros_initializer())\n",
        "   W2 = tf.get_variable(\"W2\", [num_hidden_layer_2, num_hidden_layer], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "   b2 = tf.get_variable(\"b2\", [num_hidden_layer_2, 1], initializer = tf.zeros_initializer())\n",
        "   W3 = tf.get_variable(\"W3\", [num_output_layer_1, num_hidden_layer_2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
        "   b3 = tf.get_variable(\"b3\", [num_output_layer_1, 1], initializer = tf.zeros_initializer())\n",
        "   \n",
        "   parameters = {\"W1\": W1,\n",
        "                 \"b1\": b1,\n",
        "                 \"W2\": W2,\n",
        "                 \"b2\": b2,\n",
        "                 \"W3\": W3,\n",
        "                 \"b3\": b3}\n",
        "   \n",
        "   return parameters\n",
        "\t\t\t\n",
        "def for_prop(X, parameters):\n",
        "\t\t\t\n",
        "   W1 = parameters['W1']\n",
        "   b1 = parameters['b1']\n",
        "   W2 = parameters['W2']\n",
        "   b2 = parameters['b2']\n",
        "   W3 = parameters['W3']\n",
        "   b3 = parameters['b3']\n",
        "   \n",
        "   # propagates values through NN using Rectified Linear Unit as activation function          \n",
        "   Z1 = tf.add(tf.matmul(W1, X), b1)                     \n",
        "   A1 = tf.nn.relu(Z1)                                    \n",
        "   Z2 = tf.add(tf.matmul(W2, A1), b2)                     \n",
        "   A2 = tf.nn.relu(Z2)                                   \n",
        "   Z3 = tf.add(tf.matmul(W3, A2), b3)                    \n",
        "   return Z3\n",
        "\n",
        "def c(Z3, Y):\n",
        "   logits = tf.transpose(Z3)\n",
        "   labels = tf.transpose(Y)\n",
        "   cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
        "   return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlBI9WtpPH7C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bCzNtLoPW0v"
      },
      "source": [
        "We also need to define a function that produces a random subset of observations within the training set. I mentioned previously that data is fed into the neural net until the cost value converges; each iteration a random subsample is picked out of the total training set and injected into the net. In this function we create batches of subsamples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoauZVJtPXu2"
      },
      "source": [
        "def rand_batches(X, Y, batch_size = 128, seed = 0):\n",
        "   m = X.shape[1]\n",
        "   batches = []\n",
        "   np.random.seed(seed)\n",
        "   \n",
        "   # shuffle\n",
        "   permutation = list(np.random.permutation(m))\n",
        "   shuffled_X = X[:, permutation]\n",
        "   shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "   # partition the shuffled data\n",
        "   num_batches = math.floor(m/batch_size)\n",
        "   for k in range(0, num_batches):\n",
        "       batch_X = shuffled_X[:, k * batch_size : k * batch_size + batch_size]\n",
        "       batch_Y = shuffled_Y[:, k * batch_size : k * batch_size + batch_size]\n",
        "       batch = (batch_X, batch_Y)\n",
        "       batches.append(batch)\n",
        "   \n",
        "   # handle end case\n",
        "   if m % batch_size != 0:\n",
        "       batch_X = shuffled_X[:, num_batches * batch_size : m]\n",
        "       batch_Y = shuffled_Y[:, num_batches * batch_size : m]\n",
        "       batch = (batch_X, batch_Y)\n",
        "       batches.append(batch)\n",
        "   \n",
        "   return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8b9_aIRPbF-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86123c35-a2d8-40f1-cf92-e63c64e41cfa"
      },
      "source": [
        "member_number =  12438336\n",
        "\n",
        "one = [member_number, int(member_number/5), int(member_number/100)]\n",
        "\n",
        "two = [0.02, 0.05, 0.08]\n",
        "\n",
        "a = tf.placeholder(tf.float32, shape=(3))\n",
        "\n",
        "b = tf.placeholder(tf.float32, shape=(3))\n",
        "\n",
        "result = tf.tensordot(a, b, 1)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "   print(int(result.eval(feed_dict={a: one, b: two})))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "383100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64pikpRsPwwh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwKIui7tQisM"
      },
      "source": [
        "Running the net\n",
        "\n",
        "By Dante Sblendorio\n",
        "\n",
        "In the preceding sections of this contest, we’ve gone through all the steps of installing TensorFlow, setting up a Jupyter Notebook and creating a neural net. Now it’s time for the real fun: Running the neural net and viewing our results. Keep reading for this last and final step in our journey.\n",
        "\n",
        "Now that we have defined all the functions we need, we can construct the neural net. We first initialize all the variables and parameters based on the shape of the training set, and then define the learning rate, number of epochs, and batch size. The learning rate determines how fast the mathematical operators converge on the minimum cost value. The number of epochs is the number of times training data is fed through the net. The batch size is the size of each random subsample. Each parameter has a role in the final test accuracy and how fast the net converges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl6Ni3lCQmKe"
      },
      "source": [
        "from tensorflow.python.framework import ops\n",
        "import math\n",
        "\n",
        "def nn(train_x, train_y, test_x, test_y, learning_rate ,num_epochs, batch_size, print_cost = True):\n",
        "       ops.reset_default_graph()\n",
        "       tf.set_random_seed(1)\n",
        "       (n_x, m) = train_x.shape\n",
        "       n_y = train_y.shape[0]\n",
        "       \n",
        "       # Initialize\n",
        "       costs = []\n",
        "       X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
        "       Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
        "       parameters = init_parameters(13)\n",
        "       Z3 = for_prop(X, parameters)\n",
        "       cost = c(Z3, Y)\n",
        "       optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "       init = tf.global_variables_initializer()\n",
        "\n",
        "       # Forward propagation\n",
        "       with tf.Session() as sess:\n",
        "           sess.run(init)\n",
        "\n",
        "           #training loop\n",
        "           for epoch in range(num_epochs):\n",
        "               epoch_cost = 0.\n",
        "               num_batches = int(m / batch_size)\n",
        "               batches = rand_batches(train_x, train_y, batch_size, 13)\n",
        "               for batch in batches:\n",
        "                   (batch_X, batch_Y) = batch\n",
        "                   _ , batch_cost = sess.run([optimizer, cost], feed_dict={X: batch_X, Y: batch_Y})\n",
        "                   epoch_cost += batch_cost / num_batches\n",
        "\n",
        "               # print the cost at every 50 epochs\n",
        "               if print_cost == True and epoch % 50 == 0:\n",
        "                   print (\"Epoch %i cost: %f\" % (epoch, epoch_cost))\n",
        "               if print_cost == True and epoch % 5 == 0:\n",
        "                   costs.append(epoch_cost)\n",
        "\n",
        "           # Save the parameters\n",
        "           parameters = sess.run(parameters)\n",
        "           print(\"Parameters trained...\")\n",
        "\n",
        "           # Calculate the correct predictions\n",
        "           correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "           # Accuracy of the test set\n",
        "           accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "           print(\"Train Accuracy:\", accuracy.eval({X: train_x, Y: train_y}))\n",
        "           print(\"Test Accuracy:\", accuracy.eval({X: test_x, Y: test_y}))\n",
        "\n",
        "           return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Is2E02QvyM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhS1RFmIRE4p"
      },
      "source": [
        "Now that we have defined the neural net function, we can pick values for each parameter, and run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymduuVYbRFsn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "98bdee93-23fe-425e-9146-4f498f8e5d82"
      },
      "source": [
        "learning_rate = 0.001 #change this to change learning rate\n",
        "num_epochs = 1000     #change this to change number of epochs\n",
        "batch_size = 20       #change this to change batch size\n",
        "parameters = nn(train_x, train_y, test_x, test_y, learning_rate ,num_epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 cost: 35.883492\n",
            "Epoch 50 cost: 0.522927\n",
            "Epoch 100 cost: 0.311308\n",
            "Epoch 150 cost: 0.240480\n",
            "Epoch 200 cost: 0.200274\n",
            "Epoch 250 cost: 0.171992\n",
            "Epoch 300 cost: 0.146479\n",
            "Epoch 350 cost: 0.142560\n",
            "Epoch 400 cost: 0.124002\n",
            "Epoch 450 cost: 0.114809\n",
            "Epoch 500 cost: 0.107478\n",
            "Epoch 550 cost: 0.101389\n",
            "Epoch 600 cost: 0.096205\n",
            "Epoch 650 cost: 0.091718\n",
            "Epoch 700 cost: 0.087791\n",
            "Epoch 750 cost: 0.084321\n",
            "Epoch 800 cost: 0.081231\n",
            "Epoch 850 cost: 0.078456\n",
            "Epoch 900 cost: 0.075943\n",
            "Epoch 950 cost: 0.073653\n",
            "Parameters trained...\n",
            "Train Accuracy: 0.9919355\n",
            "Test Accuracy: 0.962963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUSBDte6RLn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "c53a6a08-fb99-45e7-b81c-a275a6c26b06"
      },
      "source": [
        "learning_rate = 0.0001 #change this to change learning rate\n",
        "num_epochs = 1000     #change this to change number of epochs\n",
        "batch_size = 20       #change this to change batch size\n",
        "parameters = nn(train_x, train_y, test_x, test_y, learning_rate ,num_epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 cost: 49.291391\n",
            "Epoch 50 cost: 1.906928\n",
            "Epoch 100 cost: 1.459944\n",
            "Epoch 150 cost: 1.108970\n",
            "Epoch 200 cost: 0.878475\n",
            "Epoch 250 cost: 0.723207\n",
            "Epoch 300 cost: 0.610176\n",
            "Epoch 350 cost: 0.528843\n",
            "Epoch 400 cost: 0.468302\n",
            "Epoch 450 cost: 0.421824\n",
            "Epoch 500 cost: 0.380954\n",
            "Epoch 550 cost: 0.348661\n",
            "Epoch 600 cost: 0.321599\n",
            "Epoch 650 cost: 0.295100\n",
            "Epoch 700 cost: 0.274693\n",
            "Epoch 750 cost: 0.257200\n",
            "Epoch 800 cost: 0.239150\n",
            "Epoch 850 cost: 0.226493\n",
            "Epoch 900 cost: 0.213283\n",
            "Epoch 950 cost: 0.203568\n",
            "Parameters trained...\n",
            "Train Accuracy: 0.9354839\n",
            "Test Accuracy: 0.9444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARD0eexQRdVb"
      },
      "source": [
        "With the learning rate, number of epochs, and batch size defined as they are, the neural net does a pretty good job with the test set, accurately predicting the Class more than 96% of the time. Try changing the values of each parameter and observe how the accuracies change (it is also a good exercise to do to better understand how the neural net functions). For more on learning rate, epochs, and batch size, there is a great article to read here.\n",
        "\n",
        "To generate an entry code for Challenge 5, create a new code cell in your Jupyter notebook and paste the following code in it:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYUA3x0gRePB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e77dd4c-5933-42e1-d7ef-f1c800048243"
      },
      "source": [
        "member_number = 12438336\n",
        "\n",
        "one = [member_number, int(member_number/5), int(member_number/25), int(member_number/50), int(member_number/100)]\n",
        "\n",
        "two = [0.02, 0.05, 0.08, 0.11, 0.14]\n",
        "\n",
        "a = tf.placeholder(tf.float32, shape=(5))\n",
        "\n",
        "b = tf.placeholder(tf.float32, shape=(5))\n",
        "\n",
        "result = tf.tensordot(a, b, 1)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "   print(int(result.eval(feed_dict={a: one, b: two})))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "457730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QiVJsFgTDh_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}